# -*- coding: utf-8 -*-
"""unlearning-text-classification-inversion-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T5ucbq2a_zOfbYZ6l5bzxDmdgf36F5RX

## Mount google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers 
# !pip install ipywidgets --user
# !pip install --upgrade jupyter_client

"""## Imports and Setup"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertModel
from transformers import AutoTokenizer
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import TensorDataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.nn import functional as F
from torch.autograd import Variable
from scipy import ndimage
import copy
import random
import time
import pickle
import nltk
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

torch.set_printoptions(precision=3)
cuda = True if torch.cuda.is_available() else False
nltk.download('stopwords')
nltk.download('punkt')

# Step Load the dataset
data = pd.read_csv('/content/drive/MyDrive/unlearning-nlp/dataset/agnews.csv')
data.head()

# Remove duplicates
data.drop_duplicates(inplace=True)
data.info()

# data['label'].value_counts()

data['label'] = data['label'].apply(lambda x : x -1)
data['label'].value_counts()

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Tokenize text
    tokens = nltk.word_tokenize(text)
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    # Rejoin tokens into a single string
    text = ' '.join(tokens)
    return text


data['text'] = data['text'].apply(preprocess_text)

data = data.drop('title', axis=1)

# Comment or remove to use full dataset
selected_data = pd.DataFrame()
unique_labels = data['label'].unique()
for label in unique_labels:
    label_data = data[data['label'] == label].head(100)
    selected_data = selected_data.append(label_data)
data=selected_data
data

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Tokenize each sentence in the dataframe and encode the labels
sentences = data['text'].values
labels = data['label'].values

input_ids = []
attention_masks = []

for sent in sentences:
    encoded_dict = tokenizer.encode_plus(
                        sent,                      
                        add_special_tokens = True, 
                        max_length = 64,
                        truncation=True,          
                        padding='max_length',
                        return_attention_mask = True,   
                        return_tensors = 'pt'     
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    
    attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into PyTorch tensors
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)

# Create a TensorDataset
dataset = TensorDataset(input_ids, attention_masks, labels)

# Split the dataset into train and validation sets:

train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)

# Loaders that give 64 example batches
all_data_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,  sampler=torch.utils.data.SequentialSampler(train_dataset))
all_data_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,  sampler=torch.utils.data.SequentialSampler(test_dataset))

# Test dataloader with 3's only
threes_index = []
nonthrees_index = []
for i in range(0, len(test_dataset)):
  if test_dataset[i][2] == 3:
    threes_index.append(i)
  else:
    nonthrees_index.append(i)
three_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,
              sampler = torch.utils.data.SubsetRandomSampler(threes_index))
nonthree_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,
              sampler = torch.utils.data.SubsetRandomSampler(nonthrees_index))

# Train dataloaders with limited 3s
nonthrees_index = []
for i in range(0, len(train_dataset)):
  if train_dataset[i][2] != 3:
    nonthrees_index.append(i)
nonthree_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,
                     sampler = torch.utils.data.SubsetRandomSampler(nonthrees_index))

# Unlearning dataset with all "3" labels randomly assigned
def custom_collate(batch):
    data = [item[0] for item in batch]
    mask = [item[1] for item in batch]
    label = [item[2] for item in batch]
    data = torch.stack(data, dim=0)
    mask = torch.stack(mask, dim=0)
    label = torch.LongTensor(label)
    return data, mask, label
unlearningdata = copy.deepcopy(train_dataset)
unlearninglabels = list(range(4))
unlearninglabels.remove(3)
modified_data = []
for data,mask, label in unlearningdata:
    if label == 3:
        label = random.choice(unlearninglabels)
    modified_data.append((data,mask, label))
unlearning_train_loader = torch.utils.data.DataLoader(modified_data, batch_size=16, shuffle=True,collate_fn=custom_collate)

"""# Model"""

# Hyperparameters
batch_size_train = 16
batch_size_test = 16
log_interval = 512
num_classes = 4
torch.backends.cudnn.enabled = True
criterion = F.nll_loss

# Training method

def train(model, epoch, loader, optimizer, criterion, log_interval=10, returnable=False):
    model.train()
    if returnable:
        thracc = []
        nacc = []
    for batch_idx, (input_ids, attention_mask, target) in enumerate(loader):
        optimizer.zero_grad()
        output = model(input_ids, attention_mask)
        loss=criterion(output.logits, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print("\rEpoch: {} [{:6d}]\tLoss: {:.6f}".format(
                epoch, batch_idx*len(input_ids), loss.item()))
        if returnable and batch_idx % 10 == 0:
            thracc.append(test(model, three_test_loader, dname="Threes only", printable=False))
            if batch_idx % 10 == 0:
                nacc.append(test(model, nonthree_test_loader, dname="nonthree only", printable=False))
            model.train()
    if returnable:
        return thracc, nacc

# Testing method

def test(model, loader, dname="Test set", printable=True):
    model.eval()
    test_loss = 0
    total = 0
    correct = 0
    with torch.no_grad():
        for input_ids, attention_mask, target in loader:
            input_ids = input_ids
            attention_mask = attention_mask
            target = target
            output = model(input_ids, attention_mask)
            total += target.size()[0]
            test_loss += criterion(output.logits, target).item()
            pred = output.logits.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    test_loss /= len(loader.dataset)
    if printable:
        print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(
            dname, test_loss, correct, total, 
            100. * correct / total
            ))
    return 1. * correct / total

"""# Original Training"""

trainingepochs = 3
forgetfulepochs = 3
naive_accuracy_three = []
naive_accuracy_nonthree = []
unlearning_accuracy_three = []
unlearning_accuracy_nonthree = []

num_classes = 4
model = BertForSequenceClassification.from_pretrained('bert-base-uncased',
                                                      num_labels=num_classes,output_attentions=False,
                                                      output_hidden_states=False)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# Train new model for 5 epochs
for epoch in range(1, trainingepochs+1):
  starttime = time.process_time()
  thracc, nacc = train(model, epoch, all_data_train_loader,optimizer, criterion, returnable=True)
  naive_accuracy_three += thracc
  naive_accuracy_nonthree += nacc
  unlearning_accuracy_three += thracc
  unlearning_accuracy_nonthree += nacc
  test(model, all_data_test_loader, dname="All data")
  test(model, three_test_loader, dname="Threes  ")
  test(model, nonthree_test_loader, dname="Nonthree")
  print(f"Time taken: {time.process_time() - starttime}")

path = f"trained.pt"
torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }, path)

path = f"trained_accuracy_three.txt"
with open(path, 'w') as f:
  for data in naive_accuracy_three:
    f.write(f"{data},")

path = f"trained_accuracy_nonthree.txt"
with open(path, 'w') as f:
  for data in naive_accuracy_nonthree:
    f.write(f"{data},")

"""# Naive Retraining"""

path = f"trained.pt"

checkpoint = torch.load(path)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

naive_accuracy_three = []
naive_accuracy_nonthree =[]

# Train model for 5 epochs
for epoch in range(trainingepochs+1,trainingepochs+forgetfulepochs+1):
  thracc, nacc = train(model, epoch, nonthree_train_loader,optimizer, criterion, returnable=True)
  naive_accuracy_three += thracc
  naive_accuracy_nonthree += nacc
  test(model, all_data_test_loader, dname="All data")
  test(model, three_test_loader, dname="Threes  ")
  test(model, nonthree_test_loader, dname="Nonthree")
  path = f"retraining-epoch-{epoch}.pt"
  torch.save({ 
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }, path)

path = f"naive_accuracy_three.txt"
with open(path, 'w') as f:
  for data in naive_accuracy_three:
    f.write(f"{data},")

path = f"naive_accuracy_nonthree.txt"
with open(path, 'w') as f:
  for data in naive_accuracy_nonthree:
    f.write(f"{data},")

"""# Unlearning"""

path = f"trained.pt"

checkpoint = torch.load(path)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

unlearning_train_loader

unlearning_accuracy_three = []
unlearning_accuracy_nonthree =[]

# Train model for 5 epochs
for epoch in range(trainingepochs+1,trainingepochs+forgetfulepochs+1):
  thracc, nacc = train(model, epoch, unlearning_train_loader,optimizer, criterion, returnable=True)
  unlearning_accuracy_three += thracc
  unlearning_accuracy_nonthree += nacc
  test(model, all_data_test_loader, dname="All data")
  test(model, three_test_loader, dname="Threes  ")
  test(model, nonthree_test_loader, dname="Nonthree")
  path = f"unlearning-epoch-{epoch}.pt"
  torch.save({ 
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }, path)

path = f"unlearning_accuracy_three.txt"
with open(path, 'w') as f:
  for data in unlearning_accuracy_three:
    f.write(f"{data},")

path = f"unlearning_accuracy_nonthree.txt"
with open(path, 'w') as f:
  for data in unlearning_accuracy_nonthree:
    f.write(f"{data},")

"""# Accuracy Plotting"""

unlearning_accuracy_nonthree.insert(0,0)
unlearning_accuracy_three.insert(0,0)

divval = 200
xax = []
for i in range(len(naive_accuracy_three)):
  xax.append(i/divval)
plt.plot(xax, naive_accuracy_three, label="retraining")
xax = []
for i in range(len(naive_accuracy_nonthree)):
  xax.append(i/divval)
plt.plot(xax, naive_accuracy_nonthree, label="unlearning")
plt.axvline(x=5, ls='--', label="data removal request")
plt.legend(loc="lower left")
plt.show()

