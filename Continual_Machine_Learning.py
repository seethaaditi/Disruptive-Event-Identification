# -*- coding: utf-8 -*-
"""Old_Network_Running_Code (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D6eSv_rdcSDJp8PgX9ZsGAcsb-ujp4vh
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from torch import nn
import torch.nn.functional as F
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")0
import sklearn
from sklearn.model_selection import train_test_split
import re
import string
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk import word_tokenize
from nltk.corpus import stopwords
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from nltk.tokenize.treebank import TreebankWordDetokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer(max_features=5000)

from google.colab import drive
drive.mount('/content/drive')

# from google.colab import drive
# drive.mount('/content/drive')

df1 = pd.read_csv('NewFile.csv - Sheet1.csv')
#df2 = pd.read_csv('/content/drive/MyDrive/code/New Dataset/NEWWNewclusters_DayDataset_Feb16.csv')
#df_exemplar = pd.read_csv('/content/drive/MyDrive/code/Dataset/exemplars.csv')

df1.columns

df1.head()
#df1 = df1[['Content', 'class_label_integer']]
df1 = df1[['clean_tweet', 'class_labels_integer']]
#df1 = df1[['Review', 'Liked']]

df1.columns

#df1.class_label_integer.unique()

len(df1)

#df1=df1.drop_duplicates(subset='Content',inplace=False)
#df1=df1.drop_duplicates(subset='Text',inplace=False)

#len(df1)

#df2 = df2[['Text', 'class_labels_integer']]

#df2=df2.drop_duplicates(subset='Content',inplace=False)
#len(df2)

df1.shape

#df_exemplar.head()

#df2.shape

X = df1['clean_tweet']
y = df1['class_labels_integer']

#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, shuffle=True)

#X_train.shape, y_train.shape, X_test.shape, y_test.shape

def clean_data(text):  #function to remove punctuatuions, tokenize the text and lower the text and remove stopwords
    text_nopunct = ''
    text_nopunct = re.sub('['+string.punctuation+']', '', text)  #no punctations
    tokens = word_tokenize(text_nopunct)                       #convert into tokens
    lower_tokens = [x.lower() for x in tokens]                     #lowercase the tokens
    stoplist = stopwords.words('english')
    tokens_without_sw = [word for word in lower_tokens if not word in stopwords.words()]
    text_final = ' '.join(tokens_without_sw)

    return text_final

def clean_y(y):

  for ind,i in enumerate(y):
    if i == 1:
      y[ind] = i-1
    if i == 2:
      y[ind] = i-1
    if i == 3:
      y[ind] = i-1
    if i == 4:
      y[ind] = i-1

  return y

X = X.apply(lambda x: clean_data(x))
# X_test = X_test.apply(lambda x: clean_data(x))

len(X)

X = X.to_list()
#X_test = X_test.to_list()

y = y.to_list()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
len(X_train), len(X_test)

#y = y.to_list()
#y_test = y_test.to_list()
# y_train = clean_y(y_train)
# y_test =  clean_y(y_test)

# temp_ = df_exemplar['Text']
# temp_ = temp_.apply(lambda x: str(x).replace('<unk>',""))
# temp_ = temp_.apply(lambda x: str(x).strip()).tolist()

# lab = df_exemplar['label']
# lab = lab.tolist()
# exmp_limit = np.max(np.unique(lab))

# X = temp_ + X
# y = lab + y

numclass = len(np.unique(y_train))
numclass

train_dat = zip(X, y)
# test_dat = zip(X_test, y_test)

def yield_tokens(data_iter):
    for text,_ in data_iter:
        yield tokenizer(text)

tokenizer = get_tokenizer('basic_english')
train_iter = train_dat

vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

text_pipeline = lambda x: vocab(tokenizer(x))
label_pipeline = lambda x: int(x)

def collate_batch(batch, text_pipeline, label_pipeline):
  label_list, text_list, offset = [],[],[0]

  for (text, label) in batch:
    pro_text = torch.tensor(text_pipeline(text), dtype=torch.int64)
    text_list.append(pro_text)
    label_list.append(label_pipeline(label))
    offset.append(pro_text.size(0))

  label_list = torch.tensor(label_list, dtype=torch.int64)
  text_list = pad_sequence(text_list, batch_first=True)

  return label_list, text_list

vocab_size = len(vocab)
print(vocab_size)
#model = Network(vocab_size, emsize, num_class, hidden_dim, num_layers).to(device)
#model = Network(vocab_size, emsize, num_class, hidden_dim, num_layers)

vocab_dict = vocab.get_stoi()
#list(vocab_dict.keys())[vocab_dict.values() == 3994]

train_dat = zip(X, y)
#test_dat = zip(X_test, y_test)

train_iter2= train_dat
#test_iter2 = test_dat

train_text = [(text, label) for (text, label) in train_iter2]
#test_text = [(text, label) for (text, label) in test_iter2]

#temp_text[temp_text[0][0] == torch.tensor(2)]
#np.array(temp_text)

# lst = [temp for i,temp in enumerate(temp_text) if temp[0]==2]
# train_gen = DataLoader(lst, batch_size=32, shuffle=True)
# lst
l,t = collate_batch(train_text, text_pipeline, label_pipeline)
zip_data = zip(t, l)
_iter = zip_data
train_data = [(text, label) for (text, label) in _iter]
print(len(train_data))

# l,t = collate_batch(test_text, text_pipeline, label_pipeline)
# zip_data = zip(t, l)
# _iter = zip_data
# test_data = [(text, label) for (text, label) in _iter]
# print(len(test_data))

# import random

# random.shuffle(train_data)
# train_text = train_data[:1400]
# test_text = train_data[1400:]
train_text, test_text = train_test_split(train_data, test_size=0.2, shuffle=True)

test_text[3]

# exemplar_set = []
# for i in range(exmp_limit+1):
#   temp = [t for (t,l) in train_data if l==i]
#   exemplar_set.append(temp)

# len(exemplar_set)
#len(X)

def new_collate(text):
  label=[]
  txt=[]
  for t,l in text:
    txt.append(t)
    label.append(l)
  return txt, label
# txt, label = new_collate(test_text)
# print(txt,label)

from itertools import chain
class Dataset:

  def __init__(self, train_text, test_text, new_collate, X_train, y_train):
    #self.train_text = train_text
    # self.collate_batch = collate_batch
    #print("Thi is train text : ",train_text)
    #print('------------------------')
    self.text_pipeline = text_pipeline
    self.label_pipeline = label_pipeline
    self.train_text, self.train_label = new_collate(train_text)
    self.test_text, self.test_label = new_collate(test_text)
    self.X = X_train
    self.y = y_train


  def getTrainData(self, classes, exemplar_set):
    zip_data = zip(self.train_text, self.train_label)
    _iter = zip_data
    data = [(text, label) for (text, label) in _iter]

    datas,labels = [], []
    if len(exemplar_set)!=0:
        datas = [exemplar for exemplar in exemplar_set ]
        #print(len(exemplar_set))
        length = len(datas[0])
        #print(length)
        labels = [np.full((length), label) for label in range(len(exemplar_set))]
        datas = [j for sub in datas for j in sub]
        labels = [torch.tensor(j) for sub in labels for j in sub]

        _data = zip(datas, labels)
        _iter = _data
        datas = [tuple((data, label) for (data, label) in _iter)]



    for labe in range(classes):
      train_gen = [dat for dat in data if dat[1]==labe]
      datas.append(train_gen)
    datas = [j for sub in datas for j in sub]
    #print('again datas:', datas)
    return datas

  def getTestData(self, classes):
    datas = []
    zip_data = zip(self.test_text, self.test_label)
    _iter = zip_data
    data = [(text, label) for (text, label) in _iter]

    for label in range(classes):
      test_gen = [dat for dat in data if dat[1]==label]
      datas.append(test_gen)

    datas = [j for sub in datas for j in sub]
    return datas

  def get_text_class(self, i):
    zip_data = zip(self.train_text, self.train_label)
    _iter = zip_data
    data = [(text, label) for (text, label) in _iter]
    class_data = [dat[0] for dat in data if dat[1]==i]

    return class_data

  def get_orig_text_class(self, i):
    zip_data = zip(self.X, self.y)
    _iter = zip_data
    data = [(text, label) for (text, label) in _iter]
    class_data = [dat[0] for dat in data if dat[1]==i]

    return class_data

dataset = Dataset(train_text, test_text, new_collate, X_train, y_train)
text = dataset.get_orig_text_class(0)
len(text)

class Network(nn.Module):

    def __init__(self, vocab_size, embed_dim, num_class, hidden_dim, num_layers):
        super(Network, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=True)
        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_class)


    def forward(self, text):
        embedded = self.embedding(text) # pred_out = self.model(text)
        #packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, batch_first=True)
        out, (h0,c0) = self.lstm(embedded)
        #hidden = torch.cat((h0[-2,:,:], h0[-1,:,:]), dim = 1)
        lstm_out = h0[-1]
        #print('output after final_lstm layer: ', lstm_out)

        dense_output = self.fc(h0[-1,:,:])
        x = F.softmax(dense_output)

        return x

    def Incremental_learning(self, numclass):
        weight = self.fc.weight.data
        bias = self.fc.bias.data
        in_feature = self.fc.in_features
        out_feature = self.fc.out_features

        self.fc = nn.Linear(in_feature, numclass, bias=True)
        self.fc.weight.data[:out_feature] = weight
        self.fc.bias.data[:out_feature] = bias
        print('incremental done')

! pip install sentence-transformers
from sentence_transformers import *
import sentence_transformers

model = SentenceTransformer('paraphrase-distilroberta-base-v1')

def get_one_hot(target,num_class):
    one_hot=torch.zeros(target.shape[0],num_class)
    one_hot=one_hot.scatter(dim=1,index=target.long().view(-1,1),value=1.)
    return one_hot

class CMLmodel:

    def __init__(self, X_train, y_train, X_test, y_test, vec, numclass, vocab_size, batch_size, train_text, test_text, new_collate,
                 memory_size, epochs, learning_rate, emsize, hidden_dim, num_layers, vocab_dict,
                 model=None, exemplar_set=None):

        super(CMLmodel, self).__init__()
        self.epochs = epochs
        self.learning_rate = learning_rate
        if model == None:
          self.model = Network(vocab_size, emsize, numclass, hidden_dim, num_layers)
          self.old_model = None
        else:
          self.model = model
          self.old_model = model
        if exemplar_set == None:
          self.exemplar_set = []
        else:
          self.exemplar_set = exemplar_set
          print('exemplar_set created-----')
        self.class_mean_set = []
        self.numclass = numclass
        self.vocab_dict = vocab_dict



        self.Dataset = Dataset(train_text, test_text, new_collate, X_train, y_train)

        self.train_dataset = None
        self.test_dataset = None

        self.batchsize = batch_size
        self.memory_size = memory_size
        #self.task_size = task_size

        self.train_loader = None
        self.test_loader = None

        self.X_test = X_test
        self.y_test = y_test

        self.vec = vec.fit(X_train)


    # get incremental train data
    # incremental
    def beforeTrain(self):
        self.model.eval()
        #classes = [self.numclass - self.task_size, self.numclass]
        #print(classes)
        self.train_loader, self.test_loader = self._get_train_and_test_dataloader(self.numclass)
        if self.old_model != None:
          weight = self.model.fc.weight.data
          bias = self.model.fc.bias.data
          in_feature = self.model.fc.in_features
          out_feature = self.model.fc.out_features

          self.model.fc = nn.Linear(in_feature, numclass, bias=True)
          self.model.fc.weight.data[:out_feature] = weight
          self.model.fc.bias.data[:out_feature] = bias
          print('incremental done')
        self.model.train()
        #self.model.to(device)

    def _get_train_and_test_dataloader(self, classes):
        self.train_dataset = self.Dataset.getTrainData(classes, self.exemplar_set)
        self.test_dataset = self.Dataset.getTestData(classes)
        # self.test_dataset.getTestData(classes)
        train_loader = DataLoader(dataset=self.train_dataset,
                                  shuffle=True,
                                  batch_size=self.batchsize)

        test_loader = DataLoader(dataset=self.test_dataset,
                                 shuffle=True,
                                 batch_size=self.batchsize)

        return train_loader, test_loader


    '''
    def _get_old_model_output(self, dataloader):
        x = {}
        for step, (indexs, imgs, labels) in enumerate(dataloader):
            imgs, labels = imgs.to(device), labels.to(device)
            with torch.no_grad():
                old_model_output = torch.sigmoid(self.old_model(imgs))
            for i in range(len(indexs)):
                x[indexs[i].item()] = old_model_output[i].cpu().numpy()
        return x
    '''

    # train model
    # compute loss
    # evaluate model
    def train(self):
        accuracy = 0
        opt = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate)
        for epoch in range(self.epochs):
            for idx, (text, label) in enumerate(self.train_loader):
                #print('train-text-lenght:', text)
                loss_value = self._compute_loss(idx, text, label)
                opt.zero_grad()
                loss_value.backward()
                opt.step()
                print('epoch:%d,step:%d,loss:%.3f' % (epoch, idx, loss_value.item()))
            accuracy = self._test(self.test_loader, 1)
            print('epoch:%d,accuracy:%.3f' % (epoch, accuracy))
        return accuracy

    def _test(self, testloader, mode):
        if mode==0:
            print("compute NMS")
        self.model.eval()
        correct, total = 0, 0
        for idx, (text, label) in enumerate(testloader):
            #imgs, labels = imgs.to(device), labels.to(device)
            #print('test-text-lenght:', text)
            with torch.no_grad():
                outputs = self.model(text) if mode == 1 else self.classify(self.X_test, self.y_test)
            #print('model outputs:', outputs[5])
            predicts = torch.max(outputs, dim=1)[1] if mode == 1 else outputs
            #print('model predicts:', predicts[5])
            if mode==1:
              correct += (predicts == label).sum()
              total += len(label)
            else:
              correct += (predicts == self.y_test).sum()
              total += len(self.y_test)
        accuracy = 100 * correct / total
        self.model.train()
        return accuracy


    def _compute_loss(self, idx, text, label):

        pred_output = self.model(text)
        # print('pred_output', pred_output)
        label = get_one_hot(label, self.numclass)
        # print('target:', label)
        #output, target = output.to(device), target.to(device)
        if self.old_model == None:
            return F.binary_cross_entropy_with_logits(pred_output, label)
        else:
            #old_target = torch.tensor(np.array([self.old_model_output[index.item()] for index in indexs]))
            old_target = torch.sigmoid(self.old_model(text))
            old_task_size = old_target.shape[1]
            label[..., :old_task_size] = old_target
            return F.binary_cross_entropy_with_logits(pred_output, label)


    # change the size of examplar
    def afterTrain(self,accuracy):
        self.model.eval()
        m = int(self.memory_size / self.numclass)
        self._reduce_exemplar_sets(m)
        for i in range(self.numclass):
            print('construct class %s examplar:'%(i), end = '')
            text = self.Dataset.get_orig_text_class(i)
            #print('get_text_class ---------------------: ', text)
            self._construct_exemplar_set(text, m)

        #self.numclass += self.task_size
        self.compute_exemplar_class_mean()
        self.model.train()
        KNN_accuracy=self._test(self.test_loader,0)
        print("NMS accuracy："+str(KNN_accuracy.item()))
        filename='/content/MODELS/accuracy:%.3f_KNN_accuracy:%.3f_increment:%d_net.pkl' % (accuracy, KNN_accuracy, i + 10)
        torch.save(self.model,filename)
        self.old_model=torch.load(filename)
        #self.old_model.to(device)
        self.old_model.eval()
        #print('exp set:', self.exemplar_set)
        decoded_exemplar = []
        labels = []
        for label, example in enumerate(self.exemplar_set):
          print('----------------')
          #print('examplers :', example)
          #print('examplers shape:', len(example))
          #decoded_exmp, label = self.decode_exemplars(example, label)
          decoded_exemplar.append(example)
          label = np.ones(len(example)) * label
          labels.append(label)
        decoded_exemplar = [j for i in decoded_exemplar for j in i]
        print('----------------')
        print(decoded_exemplar)

        labels = [int(j) for i in labels for j in i]
        # print('labels :', len(labels))
        # print(labels)

        data = {'Text':decoded_exemplar,
                'label':labels}
        df1 = pd.DataFrame(data)
        df1.to_csv('/content/exemplars.csv')



    def decode_exemplars(self, exemplar, labl):
      decoded_exmp = []
      for exp in exemplar:
        exp = exp.tolist()
        tst = [key for x in exp for (key, val) in self.vocab_dict.items() if val==x]
        decoded_exmp.append(tst)
      label = np.ones(len(decoded_exmp)) * labl

      return decoded_exmp, label
      print(label)


    def _construct_exemplar_set(self, text, m):
        class_mean, feature_extractor_output = self.compute_class_mean(text)
        print('feature_extractor shape:', )
        exemplar = []
        len_of_class_samples = len(feature_extractor_output)
        #feature_extractor_output = feature_extractor_output.T
        now_class_mean = torch.zeros(feature_extractor_output.shape)
        print("now_class_mean",now_class_mean.shape)
        if m > len_of_class_samples:
          for i in range(len_of_class_samples):
              # shape：batch_size*512
              #x = class_mean - (now_class_mean + feature_extractor_output) / (i + 1)
              y = (now_class_mean + feature_extractor_output) / (i + 1)
              y = (np.transpose(y))
              #print("y",y)
              x = class_mean - y
              print('x shapeeeeeeeeeeeeeeeeeeeeeeeeeeee',x.shape)
              x = np.linalg.norm(x, axis=0)
              index = np.argmin(x)
              print('indexxxxxxxxxxxxxxxxxxxx', index)
              now_class_mean += feature_extractor_output[index]
              #print("now class mean" ,now_class_mean.shape)
              exemplar.append(text[index])
        else:
          for i in range(m):
              # shape：batch_size*512
              #x = class_mean - (now_class_mean + feature_extractor_output) / (i + 1)
              # shape：batch_size  32,10 [1,2,3,4,5,6,7,8,9,0] => 0.12
              y = (now_class_mean + feature_extractor_output) / (i + 1)
              y = (np.transpose(y))
              #print("y",y)
              x = class_mean - y
              x = np.linalg.norm(x, axis=1)
              index = np.argmin(x)
              now_class_mean += feature_extractor_output[index]
              exemplar.append(text[index])


        print("the size of exemplar :%s" % (str(len(exemplar))))
        print('+++++++++++++++++++++++++')
        print('exemplar:', exemplar)
        self.exemplar_set.append(exemplar)
        print('exemplar_set:', len(self.exemplar_set))


    def _reduce_exemplar_sets(self, m):
        for index in range(len(self.exemplar_set)):
            self.exemplar_set[index] = self.exemplar_set[index][:m]
            print('Size of class %d examplar: %s' % (index, str(len(self.exemplar_set[index]))))
            print('reduced ----')

    def compute_class_mean(self, text):
        # _loader = DataLoader(dataset=text)
        # feature_extractor_output = []
        # for i, data in enumerate(_loader):
        #   #print(data[0])
        #   feature_extractor_output.append(data[0].tolist())
        vectors = self.vec.transform(text)
        feature_extractor_output = torch.FloatTensor(vectors.toarray())
        print('feature_extractor_shape: ', feature_extractor_output.shape)
        feature_extractor_output = feature_extractor_output.T
        #print('feature_extractor_output: ', torch.tensor(feature_extractor_output[:2]))
        #print('feature_extractor_output: ', feature_extractor_output[:5])
        print('feature_extractor_output after transpose: ', feature_extractor_output.shape)
        class_mean = torch.mean(feature_extractor_output, axis=0)
        #print("class mean",class_mean)
        print('class mean shape:', class_mean.shape)

        return class_mean, feature_extractor_output

    # def decode_token(self, text):
    #   decoded_exmp = []
    #   for exp in text:
    #     #exp = exp.tolist()
    #     #print("print",exp)
    #     tst = [key for x in exp for (key, val) in self.vocab_dict.items() if val==x]
    #     #print("tst",tst)
    #     decoded_exmp.append(tst)

    #   sentences = []
    #   for i in range(len(decoded_exmp)):
    #     a = TreebankWordDetokenizer().detokenize(decoded_exmp[i])
    #     stopwords = ['<unk>']
    #     querywords = a.split()
    #     resultwords  = [word for word in querywords if word.lower() not in stopwords]
    #     result = ' '.join(resultwords)
    #     vectorizer = TfidfVectorizer()
    #     X = vectorizer.fit_transform(result)
    #     X = X.toarray()
    #     sentences.append(X)
    #   #print(sentences)

    #   return sentences

    def compute_exemplar_class_mean(self):
        self.class_mean_set = []
        for index in range(len(self.exemplar_set)):
            print("compute the class mean of %s"%(str(index)))
            exemplar=self.exemplar_set[index]
            print("exemplar",exemplar)
            # decoded_exemplar = self.decode_token(exemplar)
            # print("exemplar is", decoded_exemplar)
            # #exemplar=self.train_dataset.get_image_class(index)
            class_mean, _ = self.compute_class_mean(exemplar)
            #class_mean, _ = self.compute_class_mean(exemplar, )
            #class_mean_,_=self.compute_class_mean(exemplar,self.classify_transform)
            class_mean=(class_mean/np.linalg.norm(class_mean))
            self.class_mean_set.append(class_mean)
        #print('class_mean_set :', np.array(self.class_mean_set[1]).shape)
        print('class_mean_set shape:', (self.class_mean_set))

    def classify(self, text, label):
        result = []
        text = self.vec.transform(text)
        #print('+++++++++++++++++++++++++++++++++++++++ , ', x.toarray().shape)
        #test = F.normalize(self.model.feature_extractor(text))
        #test = self.model.feature_extractor(test).detach().cpu().numpy()
        #print('class mean----:', self.class_mean_set)
        #print('text----:', text)
        #class_mean_set = torch.tensor(self.class_mean_set)
        text = torch.FloatTensor(text.toarray())
        #text = text.T

        for target in text:
          lst = []

          for class_mean in self.class_mean_set:
            #print("class_mean_set : ",class_mean_set.shape)
            #target = np.transpose(target)
            x = target - class_mean
            #print('vec:', x)
            x = np.linalg.norm(x)
            #print('norm_vec:', x)
            lst.append(x)
          #print(lst)
          x = np.argmin(lst)
          result.append(x)
          #print('min-ind: ', x)
        #print('results:', result)
        return torch.tensor(result)

epochs = 1
learning_rate = 2
total_accuracy = None
batch_size = 32
emsize = 128
hidden_dim=10
num_layers=2
numclass = numclass
#task_size = 2
memory_size = 1000
models = None
exemplar_set = None
#exemplar_set = exemplar_set
#filename = '/content/drive/MyDrive/code/MODELS/accuracy:21.172_KNN_accuracy:36.389_increment:13_net.pkl'
#models = torch.load(filename)

model = CMLmodel(X_train, y_train, X_test, y_test, vec, numclass, vocab_size, batch_size, train_text, test_text, new_collate,
                 memory_size, epochs, learning_rate, emsize, hidden_dim, num_layers,
                 vocab_dict, models, exemplar_set)

#for i in range(2):
model.beforeTrain()
accuracy = model.train()
model.afterTrain(accuracy)
