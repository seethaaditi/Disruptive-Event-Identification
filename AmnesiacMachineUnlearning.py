# -*- coding: utf-8 -*-
"""unlearning-text-classification-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-BENqYTtG8N-RCex2h7vqBwXTZm3G7TZ

# Machine Unlearning for Text Classification

Machine unlearning for text classification involves removing outdated, inaccurate, biased, or unethical data or models from machine learning systems. This is crucial in maintaining accuracy, fairness, and ethical considerations. The unlearning process involves identifying relevant data, updating the system with new data, and retraining the model.

## Mount google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

"""## Imports and Setup"""

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertModel
from transformers import AutoTokenizer
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import TensorDataset

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch.nn import functional as F
from torch.autograd import Variable
from scipy import ndimage
import copy
import random
import time
import pickle
import nltk
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

torch.set_printoptions(precision=3)
cuda = True if torch.cuda.is_available() else False
nltk.download('stopwords')
nltk.download('punkt')

"""### Load the dataset"""

df = pd.read_csv('/content/drive/MyDrive/unlearning-nlp/dataset/agnews.csv')
df.head()

# Remove duplicates
df.drop_duplicates(inplace=True)
df.info()

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Tokenize text
    tokens = nltk.word_tokenize(text)
    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    # Rejoin tokens into a single string
    text = ' '.join(tokens)
    return text

df['text'] = df['text'].apply(preprocess_text)

df = df.drop('title', axis=1)
df.tail()

df['label'] = df['label'].apply(lambda x : x -1)
df['label'].value_counts()

selected_data = pd.DataFrame()
unique_labels = df['label'].unique()
for label in unique_labels:
    label_data = df[df['label'] == label].head(100)
    selected_data = selected_data.append(label_data)
data=selected_data
data.head()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Tokenize each sentence in the dataframe and encode the labels
sentences = data['text'].values
labels = data['label'].values

input_ids = []
attention_masks = []

for sent in sentences:
    encoded_dict = tokenizer.encode_plus(
                        sent,                      
                        add_special_tokens = True, 
                        max_length = 64,
                        truncation=True,          
                        padding='max_length',
                        return_attention_mask = True,   
                        return_tensors = 'pt'     
                   )
    
    input_ids.append(encoded_dict['input_ids'])
    
    attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into PyTorch tensors
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)

# Create a TensorDataset
dataset = TensorDataset(input_ids, attention_masks, labels)

# Split the dataset into train and validation sets:
train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)

train_dataset[110]

# Loaders that give 64 example batches
all_data_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,  sampler=torch.utils.data.SequentialSampler(train_dataset))
all_data_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,  sampler=torch.utils.data.SequentialSampler(test_dataset))



# Test dataloader with 3's only
threes_index = []
nonthrees_index = []
for i in range(0, len(test_dataset)):
  if test_dataset[i][2] == 3:
    threes_index.append(i)
  else:
    nonthrees_index.append(i)
three_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,
              sampler = torch.utils.data.SubsetRandomSampler(threes_index))
nonthree_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,
              sampler = torch.utils.data.SubsetRandomSampler(nonthrees_index))

# Train dataloaders with limited 3s
nonthrees_index = []
threes_index = []
count = 0
for i in range(0, len(train_dataset)):
  if train_dataset[i][2] != 3:
    nonthrees_index.append(i)
    threes_index.append(i)
  if train_dataset[i][2] == 3 and count < 100:
    count += 1
    threes_index.append(i)
nonthree_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,
                     sampler = torch.utils.data.SubsetRandomSampler(nonthrees_index))
three_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,
                     sampler = torch.utils.data.SubsetRandomSampler(threes_index))

# Unlearning dataset with all "3" labels randomly assigned
def custom_collate(batch):
    data = [item[0] for item in batch]
    mask = [item[1] for item in batch]
    label = [item[2] for item in batch]
    data = torch.stack(data, dim=0)
    mask = torch.stack(mask, dim=0)
    label = torch.LongTensor(label)
    return data, mask, label
unlearningdata = copy.deepcopy(train_dataset)
unlearninglabels = list(range(4))
unlearninglabels.remove(3)
modified_data = []
for data,mask, label in unlearningdata:
    if label == 3:
        label = random.choice(unlearninglabels)
    modified_data.append((data,mask, label))
unlearning_train_loader = torch.utils.data.DataLoader(modified_data, batch_size=16, shuffle=True,collate_fn=custom_collate)

# Hyperparameters
batch_size_train = 16
batch_size_test = 16
log_interval = 16
num_classes = 4
torch.backends.cudnn.enabled = True
criterion = F.nll_loss

# Training method

def train(model, epoch, loader, optimizer, criterion, log_interval=10, returnable=False):
    model.train()
    if returnable:
        thracc = []
        nacc = []
        batches = []
    for batch_idx, (input_ids, attention_mask, target) in enumerate(loader):
        optimizer.zero_grad()
        output = model(input_ids, attention_mask)
        steps = []
        if 3 in target:
            before = {}
            for param_tensor in model.state_dict():
                if "weight" in param_tensor or "bias" in param_tensor:
                    before[param_tensor] = model.state_dict()[param_tensor].clone()
        loss=criterion(output.logits, target)
        loss.backward()
        optimizer.step()
        if 3 in target:
            batches.append(batch_idx)
            after = {}
            for param_tensor in model.state_dict():
                if "weight" in param_tensor or "bias" in param_tensor:
                    after[param_tensor] = model.state_dict()[param_tensor].clone()
            step = {}
            for key in before:
                step[key] = after[key] - before[key]
            f = open(f"e{epoch}b{batches[-1]:04}.pkl", "wb")
            pickle.dump(step, f)
            f.close()
        if batch_idx % log_interval == 0:
            print("\rEpoch: {} [{:6d}]\tLoss: {:.6f}".format(
                epoch, batch_idx*len(input_ids), loss.item()), end="")
        if returnable and batch_idx % 10 == 0:
            thracc.append(test(model, three_test_loader, dname="Threes only", printable=False))
            if batch_idx % 10 == 0:
                nacc.append(test(model, nonthree_test_loader, dname="nonthree only", printable=False))
            model.train()
    if returnable:
        return thracc, nacc, batches, steps

# Testing method

def test(model, loader, dname="Test set", printable=True):
    model.eval()
    test_loss = 0
    total = 0
    correct = 0
    with torch.no_grad():
        for input_ids, attention_mask, target in loader:
            input_ids = input_ids
            attention_mask = attention_mask
            target = target
            output = model(input_ids, attention_mask)
            total += target.size()[0]
            test_loss += criterion(output.logits, target).item()
            pred = output.logits.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    test_loss /= len(loader.dataset)
    if printable:
        print('{}: Mean loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(
            dname, test_loss, correct, total, 
            100. * correct / total
            ))
    return 1. * correct / total

trainingepochs = 3
forgetfulepochs = 3
naive_accuracy_three = []
naive_accuracy_nonthree = []

# num_classes = 4
model = BertForSequenceClassification.from_pretrained('bert-base-uncased',
                                                      num_labels=num_classes,output_attentions=False,
                                                      output_hidden_states=False)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

# model

steps = []

for epoch in range(1, trainingepochs+1):
    starttime = time.process_time()
    thracc, nacc, batches, steps_epoch = train(model, epoch, three_train_loader, optimizer, criterion, returnable=True)
    naive_accuracy_three += thracc
    naive_accuracy_nonthree += nacc
    steps += steps_epoch
    print(f"{len(batches)} batches updated.")
    test(model, all_data_test_loader, dname="All data")
    test(model, three_test_loader, dname="Threes only")
    test(model, nonthree_test_loader, dname="Nonthree only")
    print(f"Time taken: {time.process_time() - starttime}")

    # Save model and accuracy values at each epoch
    # model_path = f"selective_trained_e{epoch}.pt"
    # torch.save({
    #     'model_state_dict': model.state_dict(),
    #     'optimizer_state_dict': optimizer.state_dict(),
    # }, model_path)

    # thr_acc_path = f"selective_trained_accuracy_three_e{epoch}.txt"
    # with open(thr_acc_path, 'w') as f:
    #     for data in naive_accuracy_three:
    #         f.write(f"{data},")

    # n_acc_path = f"selective_trained_accuracy_nonthree_e{epoch}.txt"
    # with open(n_acc_path, 'w') as f:
    #     for data in naive_accuracy_nonthree:
    #         f.write(f"{data},")

path = f"selective_trained.pt"
torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }, path)

# path = f"selective_trained_accuracy_three.txt"
# with open(path, 'w') as f:
#   for data in naive_accuracy_three:
#     f.write(f"{data},")

# path = f"selective_trained_accuracy_nonthree.txt"
# with open(path, 'w') as f:
#   for data in naive_accuracy_nonthree:
#     f.write(f"{data},")

path = f"selective_trained.pt"
checkpoint = torch.load(path)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

for i in range(1, trainingepochs+1):
    for j in range(1600):
        path = f"e{i}b{j:04}.pkl"
        try:
#             print("before")
            f = open(path, "rb")
            steps = pickle.load(f)
            f.close()
            print(f"\rLoading e{i}b{j:04}.pkl", end="")
            const = 1
            with torch.no_grad():
                state = model.state_dict()
                for param_tensor in state:
                    if "weight" in param_tensor or "bias" in param_tensor:
                      state[param_tensor] = state[param_tensor] - const*steps[param_tensor]
            model.load_state_dict(state)
        except:
#             print(f"\r{i},{j}", end="")
            pass

test(model, all_data_test_loader, dname="All data")
test(model, three_test_loader, dname="Threes  ")
test(model, nonthree_test_loader, dname="Nonthree")

path = f"selective_post_trained.pt"
torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            }, path)

path = f"selective_post_trained.pt"
checkpoint = torch.load(path)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

selective_post_accuracy_three = []
selective_post_accuracy_nonthree =[]

# Train model for 10 epochs
for epoch in range(trainingepochs+1,trainingepochs+forgetfulepochs+1):
  thracc, nacc, _, _ =  train(model, epoch, nonthree_train_loader, optimizer, criterion, returnable=True)
  selective_post_accuracy_three += thracc
  selective_post_accuracy_nonthree += nacc
  test(model, all_data_test_loader, dname="All data")
  test(model, three_test_loader, dname="Threes")
  test(model, nonthree_test_loader, dname="Nonthree")
#   path = f"selective-post-epoch-{epoch}.pt"
#   torch.save({ 
#             'model_state_dict': model.state_dict(),
#             'optimizer_state_dict': optimizer.state_dict(),
#             }, path)
#   path = f"selective_post_accuracy_three_e{epoch}.txt"
#   with open(path, 'w') as f:
#     for data in naive_accuracy_three:
#       f.write(f"{data},")

#   path = f"selective_post_accuracy_nonthree_e{epoch}.txt"
#   with open(path, 'w') as f:
#     for data in naive_accuracy_nonthree:
#       f.write(f"{data},")

# path = f"selective_post_accuracy_three.txt"
# with open(path, 'w') as f:
#   for data in selective_post_accuracy_three:
#     f.write(f"{data},")

# path = f"selective_post_accuracy_nonthree.txt"
# with open(path, 'w') as f:
#   for data in selective_post_accuracy_nonthree:
#     f.write(f"{data},")

